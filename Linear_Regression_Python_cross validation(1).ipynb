{
  "metadata": {
    "createdOn": 1668510319388,
    "creator": "admin",
    "customFields": {},
    "hide_input": false,
    "kernelspec": {
      "name": "py-dku-venv-datascience",
      "display_name": "Python (env datascience)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "modifiedBy": "jkbolorinos@student.ie.edu",
    "tags": []
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# LINEAR REGRESSION WITH PYTHON\n## Includes Stepwise and cross validation\n\n## Steps:\n#### 1. Initializing and data audit\n#### 2. Select relevant variables: stepwise\n#### 3. Data exploration\n#### 4. SKLEARN execution\n    A. Linear Regression with data splitting\n    B. Model Evaluation\n    C. Predictions and Error Measures\n#### 5. STATSMODEL\n    A. Linear Regression withouth data splitting\n    B. Main results\n    C. Predictions and Error Measures\n    D. Stores predictions and errors\n    E. Execute model on reserved dataset and stores predictions and errors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## Import libraries\n"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pylab inline"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import dataiku                               # Access to Dataiku datasets\nimport dataikuapi                            # API to create new datasets\nimport pandas as pd, numpy as np             # Data manipulation \nfrom dataiku import pandasutils as pdu\nfrom matplotlib import pyplot as plt         # Graphing\nimport seaborn as sns                        # Graphing\nimport statsmodels.api as sm                    # Statistical analysis\n#sns.set(style\u003d\"white\")                       # Tuning the style of charts\nimport warnings                              # Disable some warnings\nwarnings.filterwarnings(\"ignore\",category\u003dDeprecationWarning)\nfrom scipy import stats                      # Stats"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initializes variables"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_name \u003d \"wage_model\"\ndataset_name_reserved \u003d \"wage_reserved\"\ndependent \u003d \"WAGE\""
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## Check out data\n"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example: load a DSS dataset as a Pandas dataframe\nmydataset \u003d dataiku.Dataset(dataset_name)\nmydataset_df \u003d mydataset.get_dataframe()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mydataset_df.head() "
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd.set_option(\"display.precision\", 2)\nmydataset_df.describe()"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get some simple descriptive statistics\npdu.audit(mydataset_df)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stepwise execution"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Stepwise function\ndef stepwise_selection(X, y, \n                       initial_list\u003d[], \n                       threshold_in\u003d0.05, \n                       threshold_out \u003d 0.1, \n                       verbose\u003dTrue):\n    \"\"\" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value \u003c threshold_in\n        threshold_out - exclude a feature if its p-value \u003e threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in \u003c threshold_out to avoid infinite looping.\n    \"\"\"\n    included \u003d list(initial_list)\n    while True:\n        changed\u003dFalse\n        # forward step\n        excluded \u003d list(set(X.columns)-set(included))\n        new_pval \u003d pd.Series(index\u003dexcluded)\n        for new_column in excluded:\n            model \u003d sm.OLS(y, sm.add_constant(X[included+[new_column]])).fit()\n            new_pval[new_column] \u003d model.pvalues[new_column]\n        best_pval \u003d new_pval.min()\n        if best_pval \u003c threshold_in:\n            best_feature \u003d new_pval.idxmin()\n            included.append(best_feature)\n            changed\u003dTrue\n            if verbose:\n                print(\u0027Add  {:30} with p-value {:.6}\u0027.format(best_feature, best_pval))\n\n        # backward step\n        model \u003d sm.OLS(y, sm.add_constant(X[included])).fit()\n        # use all coefs except intercept\n        pvalues \u003d model.pvalues.iloc[1:]\n        worst_pval \u003d pvalues.max() # null if pvalues is empty\n        if worst_pval \u003e threshold_out:\n            changed\u003dTrue\n            worst_feature \u003d pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(\u0027Drop {:30} with p-value {:.6}\u0027.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Removes variables with missing values, stepwise does not work with them\nselected_fields\u003dmydataset_df.drop(labels\u003d[dependent], axis\u003d1)\nfor field in selected_fields:\n    if selected_fields[field].isnull().any():\n        selected_fields\u003dselected_fields.drop(labels\u003d[field], axis\u003d1)\n\nselected_fields"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sets y\ny \u003d mydataset_df[dependent]\nresult \u003d stepwise_selection(selected_fields, y)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\u0027resulting features:\u0027)\nprint(result)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Select explanatory variables"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Adds selected features from stepwise\nX\u003dselected_fields\nfor item in selected_fields.columns:\n    if item not in result:\n        X\u003dX.drop(labels\u003d[item],axis\u003d1) #removes the non relevant variables"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis\n"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Scatterplots of explanatory variables\nsns.pairplot(X)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Histogram of dependent variable\ny \u003d mydataset_df[dependent] # dependent variable\nsns.distplot(y).set(title\u003d\u0027Histogram of dependent variable\u0027)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Heatmap to show correlation between explanatory variables\nsns.set(font_scale\u003d1.1)\nfig, ax \u003d plt.subplots(figsize\u003d(10,10))         # Sample figsize in inches\nsns.heatmap(X.corr(), annot\u003dTrue, fmt\u003d\".2f\", linewidths\u003d1, ax\u003dax)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear Regression Model with SKLEARN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Test Split\n"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# cross validation\nfrom sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test \u003d train_test_split(X, y, test_size\u003d0.3)\nprint (\"Sample size train dataset: \", shape(X_train)[0])\nprint (\"Sample size test dataset: \", shape(X_test)[0])"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating and Training the Model"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.linear_model import LinearRegression\nlm \u003d LinearRegression(fit_intercept\u003dTrue)\nmodel\u003dlm.fit(X_train, y_train)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluation\n"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# print R squared train dataset\nprint(\"R2: \", model.score(X,y))"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# print coefficients\ncoeff_df \u003d pd.DataFrame(model.coef_,X.columns,columns\u003d[\u0027Coefficient\u0027])\nprint(\"Intercept: \", lm.intercept_)\ncoeff_df"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predictions from our Model\n"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# entire dataset\npredictions \u003d lm.predict(X)\nplt.title(\"Real vs. Fitted (entire dataset)\")\nplt.scatter(y,predictions)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# training dataset\npredictions_train \u003d lm.predict(X_train)\nplt.title(\"Real vs. Fitted (training dataset)\")\nplt.scatter(y_train,predictions_train, color\u003d\"green\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# test dataset\npredictions_test \u003d lm.predict(X_test)\nplt.title(\"Real vs. Fitted (test dataset)\")\nplt.scatter(y_test,predictions_test, color\u003d\"orange\")"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# combined on same chart \nplt.title(\"Real vs. Fitted (train_green vs. test_yellow datasets)\")\npredictions \u003d lm.predict(X_train)\nplt.scatter(y_train,predictions, color\u003d\"green\")\npredictions \u003d lm.predict(X_test)\nplt.scatter(y_test,predictions, color\u003d\"orange\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Residual Histograms"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# entire dataset\n#sns.distplot((y-predictions),bins\u003d50)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "warnings.filterwarnings(\"ignore\")\n\n# training dataset\nsns.distplot((y_train-predictions_train),bins\u003d50, color\u003d\"green\").set(title\u003d\u0027Histogram of dependent - train dataset\u0027)"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# test dataset\nsns.distplot((y_test-predictions_test),bins\u003d50, color\u003d\"orange\").set(title\u003d\u0027Histogram of dependent - test dataset\u0027)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regression Evaluation Metrics\n\n\nHere are three common evaluation metrics for regression problems:\n\n**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n\n$$\\frac 1n\\sum_{i\u003d1}^n|y_i-\\hat{y}_i|$$\n\n**Mean Squared Error** (MSE) is the mean of the squared errors:\n\n$$\\frac 1n\\sum_{i\u003d1}^n(y_i-\\hat{y}_i)^2$$\n\n**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n\n$$\\sqrt{\\frac 1n\\sum_{i\u003d1}^n(y_i-\\hat{y}_i)^2}$$\n\n**Mean Absolute Percentage Error** (MAPE) is the mean of the absolute percent error:\n\n$${\\frac 1n\\sum_{i\u003d1}^n}{|y_i-\\hat{y}_i)|\\over y_i}$$\n\nComparing these metrics:\n\n- **MAE** is the easiest to understand, because it\u0027s the average error.\n- **MSE** is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\n- **RMSE** is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\n- **MAPE** is the best metric, because allows comparison between different models.\n\nAll of these are **loss functions**, because we want to minimize them.\n"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn import metrics"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# entire dataset\n#print(\u0027MAE:\u0027, metrics.mean_absolute_error(y, predictions))\n#print(\u0027MSE:\u0027, metrics.mean_squared_error(y, predictions))\n#print(\u0027RMSE:\u0027, np.sqrt(metrics.mean_squared_error(y, predictions)))\n#print(\u0027MAPE:\u0027, np.mean(100*abs(y-predictions)/y))"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# error comparison between the two datasets\nprint (\"Errors:\\n\")\nprint(\u0027MAE (train):\u0027, metrics.mean_absolute_error(y_train, predictions_train))\nprint(\u0027MAE (test):\u0027, metrics.mean_absolute_error(y_test, predictions_test))\n\nprint(\u0027MSE (train):\u0027, metrics.mean_squared_error(y_train, predictions_train))\nprint(\u0027MSE (test):\u0027, metrics.mean_squared_error(y_test, predictions_test))\n\nprint(\u0027RMSE (train):\u0027, np.sqrt(metrics.mean_squared_error(y_train, predictions_train)))\nprint(\u0027RMSE (test):\u0027, np.sqrt(metrics.mean_squared_error(y_test, predictions_test)))\n\nprint(\u0027MAPE (train):\u0027, np.mean(100*abs(y_train-predictions_train)/y_train))\nprint(\u0027MAPE (test):\u0027, np.mean(100*abs(y_test-predictions_test)/y_test))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# ADITIONAL INFO USING STATSMODELS LIBRARY\n### (It does not include cross validation)"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# model with an additional library (statmodels)\nx \u003d sm.add_constant(X) "
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result \u003d sm.OLS(y, x).fit()\nprint (result.summary())"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#move dependent variable to the end to show it along with predictions and errors\nmydataset_df \u003d mydataset_df[[col for col in mydataset_df.columns if col !\u003ddependent] + [dependent]]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#create predictions and convert them into a dataframe \npredictions \u003d result.predict()\nmydataset_df[dependent + \"_predicted\"] \u003d pd.DataFrame(predictions, columns \u003d [dependent + \u0027_predicted\u0027])"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# calculates error, %error and MAPE\nprint (\"Errors:\\n\")\nmydataset_df[\"error\"] \u003d mydataset_df[dependent] - mydataset_df[dependent + \"_predicted\"]\nmydataset_df[\"%abs error\"] \u003d abs(100*mydataset_df[\"error\"]) /  mydataset_df[dependent]\nprint(\u0027MAE:\u0027, abs(mydataset_df[\"error\"]).mean())\nMSE \u003d np.square(mydataset_df[\"error\"]).mean()\nprint(\u0027MSE:\u0027, MSE) \nprint(\u0027RMSE:\u0027, np.sqrt(MSE))\nprint (\"MAPE: \", mydataset_df[\"%abs error\"].mean())\n\nmydataset_df"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stores predictions and errors in new dataset"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Workaraound in dataiku to store results into a new dataset (not available with free license)\ndef create_dataset(dataset_name, df):\n    client \u003d dataiku.api_client()\n    project \u003d client.get_project(dataiku.default_project_key())\n    project_variables \u003d dataiku.get_custom_variables()\n    csv_dataset_name \u003d dataset_name + \"_results\"\n\n    # Create dataset if it doesn\u0027t already exist\n    try:\n        # If dataset exists, clear it\n        csv_dataset \u003d project.get_dataset(csv_dataset_name) # doesn\u0027t generate error if dataset doesn\u0027t exist\n        csv_dataset.clear()\n    except:\n        # Create dataset (assuming exception was that dataset does not exist)\n        params \u003d {\u0027connection\u0027: \u0027filesystem_folders\u0027, \u0027path\u0027: project_variables[\u0027projectKey\u0027]  + \u0027/\u0027 + csv_dataset_name}\n        format_params \u003d {\u0027separator\u0027: \u0027\\t\u0027, \u0027style\u0027: \u0027unix\u0027, \u0027compress\u0027: \u0027\u0027}\n\n        csv_dataset \u003d project.create_dataset(csv_dataset_name, type\u003d\u0027Filesystem\u0027, params\u003dparams, formatType\u003d\u0027csv\u0027, formatParams\u003dformat_params)\n\n        # Set dataset to managed\n        ds_def \u003d csv_dataset.get_definition()\n        ds_def[\u0027managed\u0027] \u003d True\n        csv_dataset.set_definition(ds_def)\n\n    # Save results into new dataset\n    output \u003d dataiku.Dataset(csv_dataset_name)\n    output.write_with_schema(df)    "
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Creates model dataset with results\ncreate_dataset(dataset_name, mydataset_df)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate predictions and errors from the reserved dataset"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mydataset2 \u003d dataiku.Dataset(dataset_name_reserved)\nmydataset_df2 \u003d mydataset2.get_dataframe()\nmydataset_df2 \u003d mydataset_df2[[col for col in mydataset_df2.columns if col !\u003ddependent] + [dependent]]"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create predictions\ni\u003d0\nformula\u003dresult.params[0]\nfor param in result.params[1:]:\n    formula +\u003d param*mydataset_df2[X.columns[i]]\n    i+\u003d1\n\n# Store predictions end errors   \nmydataset_df2[dependent + \"_predicted\"] \u003d formula\nmydataset_df2[\"error\"] \u003d mydataset_df2[dependent] - mydataset_df2[dependent + \"_predicted\"]\nmydataset_df2[\"%abs error\"] \u003d abs(100*mydataset_df2[\"error\"]) /  mydataset_df[dependent]\n\n# Display errors    \nprint (\"Errors from the reserved dataset:\\n\")\nprint(\u0027MAE:\u0027, abs(mydataset_df2[\"error\"]).mean())\nMSE \u003d np.square(mydataset_df2[\"error\"]).mean()\nprint(\u0027MSE:\u0027, MSE) \nprint(\u0027RMSE:\u0027, np.sqrt(MSE))\nprint (\"MAPE: \", mydataset_df2[\"%abs error\"].mean())"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Creates reserved dataset with results\ncreate_dataset(dataset_name_reserved, mydataset_df2)"
      ],
      "outputs": []
    }
  ]
}